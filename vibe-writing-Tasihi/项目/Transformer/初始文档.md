> 这篇文档是关于Transformer的基础梳理。Transformer是当前几乎所有大语言模型（GPT、Claude、Gemini、LLaMA等）的底层架构。理解它，就是理解AI为什么能"读懂"和"生成"语言。

## Transformer从哪来？
2017年，Google的一篇论文《Attention Is All You Need》提出了Transformer架构。在它之前，处理语言的主流方法是RNN（循环神经网络）——一个词一个词地顺序处理，像人逐字阅读一样。
RNN的问题很直接：它必须一个一个来。处理到第100个词时，第1个词的信息已经衰减得很厉害了。而且因为顺序处理，无法并行计算，训练速度慢。
Transformer的核心主张很激进：**完全抛弃顺序处理，用注意力机制一次性看完所有词**。这就是论文标题的含义——"注意力就是你所需要的一切"。

## 注意力机制：Transformer的核心引擎
### 什么是自注意力？
想象你在读一句话："小猫坐在垫子上，因为**它**很累。"
人类读到"它"的时候，大脑会自动回溯——"它"指的是"小猫"，不是"垫子"。
自注意力做的就是这件事：让每个词去"看"句子里所有其他词，算出跟谁关系最大，然后把相关信息汇总过来。
[[vibe-writing-Tasihi/项目/Transformer/知识卡片/01. 注意力机制到底在干什么？Q、K、V是怎么配合的？|01. 注意力机制到底在干什么？Q、K、V是怎么配合的？]]
### Q、K、V三个角色
自注意力的实现靠三个向量：
- **Query（查询）**：这个词在"问"——"谁跟我有关？"
- **Key（键）**：这个词在"回答"——"我这里有什么信息"
- **Value（值）**：这个词提供的实际内容——"如果你需要我，这是我能给你的"
每个词都同时拥有Q、K、V三个角色。计算过程：
1. 用每个词的Query去点乘所有词的Key → 得到关联分数
2. 把分数除以√d（缩放，防止数值太大）
3. Softmax归一化 → 变成0到1的权重（加起来等于1）
4. 用权重去加权所有词的Value → 得到这个词的最终表示
公式：**Attention(Q, K, V) = softmax(QK^T / √d_k) · V**
[[vibe-writing-Tasihi/项目/Transformer/知识卡片/02. W矩阵是随机开始的，凭什么最后能处理没见过的句子？|02. W矩阵是随机开始的，凭什么最后能处理没见过的句子？]]
### 多头注意力：同时从多个角度看
一次注意力只能捕捉一种关系。但语言里的关系是多层次的——语法关系、语义关系、指代关系、逻辑关系……
多头注意力就是把Q、K、V拆成多组（比如8组或16组），每组独立算注意力，最后把结果拼接起来。相当于让模型同时从多个角度理解词之间的关系。

## Transformer的整体结构
### 编码器-解码器架构
原始Transformer是一个**编码器-解码器**结构：
- **编码器**：读入完整的输入句子，生成每个词的深层表示
- **解码器**：基于编码器的输出，一个词一个词地生成翻译/回答
编码器和解码器各自有6层（可以更多），每层包含：
1. 多头自注意力层
2. 前馈神经网络层（两层线性变换 + ReLU激活）
3. 残差连接 + 层归一化（让深层网络能训练）
### 解码器的特殊之处：掩码
解码器在生成第N个词时，不能偷看第N+1个词。所以它用了**掩码注意力**——把未来位置的分数设为负无穷，softmax后变成0，确保只能看到已经生成的内容。

## 位置编码：告诉模型词的顺序
自注意力有个天然缺陷：它不知道词的顺序。"猫追狗"和"狗追猫"在纯注意力看来没区别。
解决方案是**位置编码**——给每个位置生成一个固定的向量，加到词向量上。原始论文用的是正弦/余弦函数，后来出现了很多改进方案（RoPE、ALiBi等），让模型能处理更长的文本。

## 为什么Transformer赢了？
### 并行计算
RNN必须一个词一个词处理，Transformer一次性处理所有词。在GPU上，这意味着训练速度快了几个数量级。
### 长距离依赖
RNN中，第1个词的信息要经过99步才能到第100个词，信息衰减严重。Transformer中，任意两个词之间只有1步——直接通过注意力连接。
### 可扩展性
Transformer的结构非常规整，容易堆叠更多层、用更多数据训练。这就是为什么大语言模型能从1亿参数扩展到上万亿参数。

## 当前的变体和演进
### 三大流派
1. **编码器型**（BERT系列）：只用编码器，擅长理解任务（分类、提取）
2. **解码器型**（GPT、Claude、LLaMA系列）：只用解码器，擅长生成任务
3. **编码器-解码器型**（T5、BART）：两者都用，擅长翻译和摘要
目前最主流的大语言模型几乎全部采用**解码器型**架构。
### 效率改进
原始注意力的计算量是O(n²)——序列长度翻倍，计算量翻四倍。针对这个问题，出现了很多改进：
- **FlashAttention**：不改变数学计算，只优化GPU内存访问模式，大幅加速
- **分组查询注意力（GQA）**：多个Query共享同一组Key和Value，减少计算量
- **滑动窗口注意力**：每个词只看附近的词，处理超长文本时更高效
### 位置编码的进化
- **RoPE（旋转位置编码）**：把位置信息编码为旋转，支持外推到更长序列
- **ALiBi**：不加位置向量，而是给远距离的注意力分数加上负偏置

## 还有哪些开放问题？
1. **为什么Transformer这么好用？** 理论上的完整解释至今还在研究中
2. **注意力的O(n²)能不能从根本上解决？** 状态空间模型（如Mamba）在尝试用线性复杂度替代注意力
3. **Transformer的极限在哪？** 参数规模还能继续增长吗？会不会遇到收益递减？
4. **多模态**：Transformer已经从文本扩展到图像（ViT）、音频、视频，统一架构的趋势会走向哪里？
