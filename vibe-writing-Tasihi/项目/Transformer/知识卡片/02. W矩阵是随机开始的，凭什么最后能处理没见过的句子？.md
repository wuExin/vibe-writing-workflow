> Q、K、V的计算依赖三个权重矩阵W_Q、W_K、W_V，但这些矩阵是哪来的？如果是训练出来的，训练完就固定了，那面对从没见过的新句子，模型怎么还能正确分配注意力？

## W矩阵的来历：从随机噪声到通用规则
W_Q、W_K、W_V一开始全是随机数。这时候模型算出来的注意力权重也是随机的——可能让"它"最关注"在"，完全不关注"小猫"。
训练过程就是反复试错：
1. 给模型一个任务，比如挖掉一个词让它猜——"小猫坐在___上"
2. 模型猜一个答案（比如"天空"）
3. 通过损失函数算出猜得有多离谱
4. 反向传播：顺着计算链条反推每个参数对错误的"贡献度"（梯度）——是W_Q把Query变成了错误的方向，还是W_K把Key变成了错误方向？
5. 每个参数朝着"减少错误"的方向微调一小步
重复几十亿次，每次用不同的句子。W_Q逐渐学会"把词向量变成好的提问方向"，W_K学会"把词向量变成好的标签方向"。
### 没有人教它规则
没有人告诉模型"代词应该关注名词"。模型是从数据中自己发现的——每次"它"不关注"小猫"就会猜错，反复惩罚之后，W_Q和W_K自然调整到让代词的Query和名词的Key高度匹配。

## 泛化能力：学到的是规律，不是句子
训练完W就固定了，但面对新句子照样能正确分配注意力。原因跟你学开车一样——你练的是具体几条路线，但学到的是"看到红灯就停、看到路牌就判断方向"这种通用规则，所以能开任何路线。
模型在训练中见过海量结构相似的句子：
- "小猫坐在垫子上，因为它很累"
- "那辆车停在路边，因为它没油了"
- "这本书放在桌上，因为它太重了"
表面内容不同，但结构一样——"[名词A]...[名词B]...因为它..."。W学到的不是"它→小猫"这个具体对应，而是：当代词出现在"因为"之后，它的Query应该跟前面主语名词的Key高度匹配。
所以遇到从没见过的"那台服务器挂在机架上，因为它太热了"，模型依然让"它"关注"服务器"。
### 泛化的三个层次
**词级泛化**：词向量本身带有语义信息。"服务器"和"电脑"的向量很接近，W作用在它们上面产生的Q、K也接近。见过"电脑"的句子，面对"服务器"时自然能处理。
**结构级泛化**：W学到的是"什么类型的词在什么语境下应该关注什么类型的词"——一种不依赖具体词汇的模式识别。
**组合泛化**：语言是组合性的，有限规则产生无限句子。分别见过"猫在垫子上"和"服务器很热"，就能处理"猫很热"或"服务器在机架上"，因为掌握的是可组合的规则。
### 泛化也会失败
如果新句子的结构跟训练数据差异太大——极罕见的语言结构、隐喻、对抗性句子——注意力就可能分配错。这也是大语言模型仍会"犯蠢"的原因之一：不是模型不够大，而是某些模式在训练数据中太稀少。

> W矩阵从随机出发，通过海量试错学到的是语言的结构性规律而非具体句子。这解释了Transformer为什么能"以有限应对无限"——也揭示了它的边界：泛化能力再强，也受限于训练数据中模式的覆盖范围。
